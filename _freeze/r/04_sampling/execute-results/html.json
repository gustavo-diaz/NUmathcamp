{
  "hash": "0056b1015742ffc47a8fbf5cd162ea50",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sampling and simulation\"\nauthor: \"[Artur Baranov](https://artur-baranov.github.io)\"\ndate: 2024-09-20\nformat: \n  html:\n    embed-resources: true\ntoc: true\n---\n\n\n## Before we start\n\n-   What is `%>%`?\n\n-   A clear environment = a clear mind!\n\n-   How to create a chunk?\n\n-   How to run a line of code?\n\n\n<center>\n<a href=\"https://artur-baranov.github.io/data/04_sampling.qmd\" class=\"btn btn-primary\" role=\"button\" download=\"04_sampling.qmd\" style=\"width:200px\" target=\"_blank\">Download script</a>\n</center>\n\n\n## Random sampling from distributions\n\nGenerating data is a powerful skill used to model different situations, allowing for statistical inference. For example, based on distributions we can predict the probability of a specific value occurring.\n\n### Uniform distribution \n\nSimple things first. For example, you have a task of generating random number between 0 and 1. How would computer do it? Random number generation is a slightly counter intuitive process. Basically, computer is extracting numbers from a **uniform distribution.** Let's try it out.\n\nRun this chunk a couple of times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunif(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.4696058 0.6359545 0.2781490 0.2525799 0.1566532 0.2139058 0.3334451\n [8] 0.8963178 0.8623301 0.4870929\n```\n\n\n:::\n:::\n\n\nIf you'd like to control the random generation process, you can use `set.seed()` function. Try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nrunif(10)\n\nset.seed(123)\nrunif(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\n```\n\n\n:::\n:::\n\n\nYou can adjust the boundaries however you would like to. Let's simulate a million of observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nuniform_data <- runif(n = 1000000, min = -3, max = 3)\n```\n:::\n\n\nNow, visualize what we got. It looks like a rectangle! In other words, every number is equally likely to appear. And that's what the uniform distribution is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nggplot(data.frame(uniform_data), aes(x = uniform_data)) +\n  geom_histogram(binwidth = 0.25, boundary = 0, closed = \"right\") +\n  scale_x_continuous(breaks = seq(-5, 5, 1), limits = c(-5, 5))\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nLet's consider a less theoretical example. You'd like to assign a student for next week's presentation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudents_list <- c(\"student_1\", \"student_2\", \"student_3\", \"student_4\", \"student_5\", \"student_6\")\n```\n:::\n\n\nTo do this, you can use `sample()` function. You would like each student to appear with the same probability as any other student.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(students_list, size = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"student_5\"\n```\n\n\n:::\n:::\n\n\nHow fair is this random assignment? You can test it using R! To do this, we need to collect data. In our example, we can generate results easily. Take a look at how we modified `sample()`. `replace = T` argument puts the student back after they are drawn.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatabase <- data.frame(\"student\" = sample(students_list, size = 10000, replace = T))\nhead(database)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    student\n1 student_4\n2 student_4\n3 student_3\n4 student_2\n5 student_2\n6 student_4\n```\n\n\n:::\n:::\n\n\nNow, check out the graph! Thus, *without adjustment*, `sample()` essentially follows a uniform distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(database) +\n  geom_histogram(aes(x = student), stat = \"count\") \n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n### Normal distribution (Continuous Data)\n\nHowever, most numbers that exist in the world tend to have higher probabilities around certain values—almost like gravity around a specific point. For instance, income in the United States is not uniformly distributed—a handful of people are really really rich, lots are very poor, and most are kind of clustered around an average. \n\nOne of such distributions is the “bell curve” - or normal distribution. Let's generate a normal distribution data and visualize it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nnormal_data <- rnorm(1000)\n\nnormal_plot <- ggplot(data.frame(normal_data), aes(x = normal_data)) +\n  geom_histogram(color = \"white\") \nnormal_plot\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\nNow, let's calculate and plot the average of our data. It's centered at 0! This is the first **parameter** of the normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_plot + geom_vline(xintercept = mean(normal_data), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nBut there is another **parameter** of the normal distribution which is the standard deviation (sd or $\\sigma$). In simple terms, it means the spread of data from the average. For example, let's highlight +- 1 sd from the average. Generally, it covers 68% of observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_plot + \n  geom_vline(xintercept = mean(normal_data) + sd(normal_data), color = \"blue\") +\n  geom_vline(xintercept = mean(normal_data) - sd(normal_data), color = \"blue\") \n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nIn statistics we are often interested in finding a \"location\" of 95% of observations. It is roughly within +- 2 sd.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_plot + \n  geom_vline(xintercept = mean(normal_data) + 2*sd(normal_data), color = \"blue\") +\n  geom_vline(xintercept = mean(normal_data) - 2*sd(normal_data), color = \"blue\") \n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n#### Probability Density Function\n\nMathematicians - as usual - came up with formula to represent this kind of distribution abstractly. I don't want to scare you, so we won't consider the formula here. However, this formula allows us to draw the graph below. This is a generalized version of the normal distribution, it's called **Probability Density Function (PDF)**. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = dnorm) +\n  geom_vline(aes(xintercept = 1, color = \"1 SD\")) +\n  geom_vline(aes(xintercept = -1, color = \"1 SD\")) +\n  geom_vline(aes(xintercept = 2, color = \"2 SD\")) +\n  geom_vline(aes(xintercept = -2, color = \"2 SD\")) +\n  labs(title = \"Normal Distribution (mean = 0, sd = 1)\",\n       subtitle = \"Probability Density Function\",\n       color = \"\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = seq(-5, 5, by = 1), limits = c(-5, 5))\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nUsing this, we can, say, calculate the probability of drawing \"0\" from a normal distribution with $\\mu = 0$ and $\\sigma = 1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnorm(0, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3989423\n```\n\n\n:::\n:::\n\n\nLet's check it on our `normal_data` we generated previously. I suggest changing `binwidth` for straightforward calculation. Does it remind you the **PDF**?\n\n*Do you remember what the binwidth argument does?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_plot +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(breaks = seq(-5, 5, by = 1), limits = c(-5, 5))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThere are slightly less than 40% of observations fall into \"0\". It matches our expectations! In other words, knowing the average and the standard deviation of the normal distribution, we can predict how many observations would be of given value. \n\n#### Cumulative Distribution Function\n\nWhat if we are interested in how many observations would be 0 or fewer? For this option we might use **Cumulative Distribution Function**. \n\n*Any guesses at this point? Take a look on the PDF!*\n\nCumulative distribution function looks like this. This is kind of S shape.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_function(fun = pnorm) +\n  xlim(-5,5) +\n  labs(title = \"Normal Distribution (mean = 0, sd = 1)\",\n       subtitle = \"Cumulative Distribution Function\",\n       y = \"Probability\") \n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nSlightly harder is to calculate more precise values. For example, how many observations would be 1 or less? 84%!\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(1, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8413447\n```\n\n\n:::\n:::\n\nOn the graph, it would look like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(NULL, aes(c(-5,5))) +\n  geom_area(stat = \"function\", fun = dnorm,  xlim = c(-1.35, 5)) +\n  geom_area(stat = \"function\", fun = dnorm, fill=\"sky blue\", xlim = c(-5, 1)) +\n  labs(title = \"Normal Distribution (mean = 0, sd = 1)\",\n       x = \"\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = seq(-5, 5, by = 1), limits = c(-5, 5))\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nConversely, to know how many observation would be 1 or more we can use a trick:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(1, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1586553\n```\n\n\n:::\n:::\n\n\n### Binomial distribution (Discrete Data)\n\nOften you’ll want to generate a column that only has two values: yes/no, treated/untreated, before/after, big/small, red/blue, etc. You’ll also likely want to control the proportions (25% treated, 62% blue, etc.).\n\nFor instance, we can ask R to do the following twenty times: flip a fair coin one hundred times, and count the number of tails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n = 20, size = 100, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 51 45 54 54 43 57 55 49 44 54 46 52 46 54 47 45 51 52 52 53\n```\n\n\n:::\n:::\n\n\nWith `prob =` , we can implement unfair coins:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n = 20, size = 100, prob = 0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 94 90 87 90 94 92 93 84 92 91 93 95 93 94 86 93 90 87 92 89\n```\n\n\n:::\n:::\n\n\nR will choose these values with equal/uniform probability by default, but you can change that in `sample()` with the `prob` argument. For instance, pretend you want to simulate an election. According to the latest polls, one candidate has an 80% chance of winning. You want to randomly choose a winner based on that chance. Here’s how to do that with `sample()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ncandidates <- c(\"Person 1\", \"Person 2\")\nfake_elections <- data.frame(winner = sample(candidates,\n                                         size = 1000,\n                                         prob = c(0.8, 0.2),\n                                         replace = TRUE))\n\n\nggplot(fake_elections, aes(x = winner)) +\n  geom_histogram(stat = \"count\")\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n### Other distribution functions\n\nEach distribution can do more than generate random numbers (the prefix r). We can compute the cumulative probability by the function `pbinom()`, `punif()`, and `pnorm()`. Also the density – the value of the PDF – by `dbinom()`, `dunif()` and `dnorm()`.\n\n\n## Random Sampling from Data\n\nLet's work with the real data a bit. Download results of World Happiness Report 2024 from the following [URL](https://github.com/gustavo-diaz/NUmathcamp/blob/main/r/data/happiness_report_2024.csv). Load it to R!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report <- read.csv(\"data/happiness_report_2024.csv\")\n```\n:::\n\n\nVisualize the `happiness_score` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(h_report, aes(x = ...)) +\n  geom_histogram(color = \"white\", binwidth = 1) +\n  labs(title = \"Distribution of happiness_score variable\") +\n```\n:::\n\n\nNow, let's sample 80 observations from the dataset. Basically, we are randomly extracting rows from the main dataset. Compare `sample()` and `sample_n()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nh_report_sample <- sample_n(h_report, size = 80)\n```\n:::\n\n\nNow, compare the graphs of original data and of its sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(h_report_sample, aes(x = happiness_score)) +\n  geom_histogram(color = \"white\", binwidth = 1) +\n  labs(title = \"Distribution of Sample of happiness_score variable\")\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nWe can check basic characteristics of those distributions. For example, their average. It's pretty similar!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(h_report_sample$happiness_score)\nmean(h_report$happiness_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.487167\n[1] 5.527572\n```\n\n\n:::\n:::\n\n\nAlternatively, we can check standard deviation. They are quite similar too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(h_report_sample$happiness_score)\nsd(h_report$happiness_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.210437\n[1] 1.17069\n```\n\n\n:::\n:::\n\n\nIf we assume that true average $\\mu$ is equal to 5.53, then our estimator $\\hat{\\mu}$ from the average of the data $\\bar{X}$ would be 5.49. In other words, even with the limited access to data we were able to figure average of the \"real\" distribution.\n\nNow, briefly. Let's check how it matches with our *theoretical* assumptions of how the world works. We assume that the happiness scores are distributed normally. Thus, even with a *limited* sample knowledge (N=80) we can attempt to generalize on the whole population (N = 143). Let's, say, calculate the probability that a given country would receive 6 on happiness score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnorm(x = 6,\n      mean = mean(h_report_sample$happiness_score),\n      sd = sd(h_report_sample$happiness_score))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3012936\n```\n\n\n:::\n:::\n\nNow, let's calculate the real data. To make things easier, we add a count of countries in each bin.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(h_report, aes(x = happiness_score)) +\n  geom_histogram(color = \"white\", binwidth = 1) +\n  stat_bin(binwidth = 1, geom = \"text\", aes(label = ..count..), vjust = -0.5)\n```\n\n::: {.cell-output-display}\n![](04_sampling_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\nWe got $\\approx$ 0.34, when we predicted 0.30. Sounds good with a limited access to data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n49/143\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3426573\n```\n\n\n:::\n:::\n\n\n::: callout-note\n## Exercise\n\nYou have access to `diamonds` dataset. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(diamonds)\n```\n:::\n\n\nWhat are the variables in this dataset?\n\n\n::: {.cell}\n\n:::\n\nUsing `ggplot()`, draw a histogram of the variable `depth`. Diamond depth describes its proportions. The less, the better. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(diamonds, aes(x=...))+\n  geom_histogram() \n```\n:::\n\nDoes it remind you of normal distribution?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(diamonds$depth)\nsd(diamonds$depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 61.7494\n[1] 1.432621\n```\n\n\n:::\n:::\n\nDraw a probability density function (`dnorm()`). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = ..., args = list(mean = mean(...), sd = sd(...))) +\n  xlim(40, 80)\n```\n:::\n\nSample 1000 observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ndiamonds_sample = sample_n(diamonds, size = 1000)\n```\n:::\n\n\nCompare average and standard deviation. Are they comparable with the original data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(...)\nsd(...)\n```\n:::\n\nIn your free time, feel free to experiment how few observations you need to get more or less reliable parameters of a normal distribution!\n\n:::\n\n\n### Bootstrapping\n\nTo get a better measure of $\\hat{\\mu}$ we can apply bootstrapping. To put it simple, we are going to repeat sampling process several times, and each time we draw random sample from the dataset we record its average. First, we create a dataset which we'll be using to store those averages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_averages <- data.frame()\n```\n:::\n\n\nUsing loops, you can repeat actions several times. This is what programming is quite helpful in! Explore how a `for` loop below works.\n\n-   `for` indicates the beginning of the loop\n\n-   `i` stands for the index of an iteration\n\n-   `in 1:10`  specifies the range of values from 1 to 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:10){\n  print(i)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n\n\n:::\n:::\n\n\nNow, iterate (i.e. repeat) sampling process!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:100){\n  temporary_sample <- sample_n(h_report, size = 80)                   # sample 80 observations\n  temporary_sample_average <- mean(temporary_sample$happiness_score)  # calculate the sample average \n  sample_averages <- rbind(sample_averages, temporary_sample_average) # add the sample average to the df\n}\n\ncolnames(sample_averages) = \"average\"\nhead(sample_averages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   average\n1 5.444772\n2 5.782780\n3 5.351790\n4 5.519500\n5 5.477656\n6 5.479902\n```\n\n\n:::\n:::\n\n\nTake a look on the average of the collected averages. Did it get closer to the real parameter? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sample_averages$average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.531569\n```\n\n\n:::\n:::\n\n\n::: callout-note\n## Exercise\n\nIn dataframe `h_report`, explore `freedom` variable. Check the descriptive statistics.\n\nDraw a histogram of this variable using ggplot. Indicate the average\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(...) +\n  geom_histogram(aes(x = ...)) +\n  geom_vline(xintercept = mean(..., na.rm = T))\n```\n:::\n\n\nCalculate the average.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(..., na.rm = T)\n```\n:::\n\n\nUsing bootstrapping, get the comparable average.\n\nFirst, create a dataframe to store the results of the averages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootsrap_averages = ...\n```\n:::\n\n\nFinish this `for` loop so it saves average of sampled data. Repeat the loop 10 times. Limit the size of `sample_n` to 50.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n...(i in 1:...){\n  temporary_sample <- sample_n(h_report, size = ...)\n  temporary_sample_average <- ...\n  bootsrap_averages <- rbind(bootsrap_averages, temporary_sample_average)\n}\n\ncolnames(bootsrap_averages) = \"average\"\n```\n:::\n\n\nCompare the average of `h_report$freedom` and `bootsrap_averages$average`. Are they different? \n\n\n::: {.cell}\n\n:::\n\n\nRepeat the comparison, but change the number of iterations to 100. Did the result improve?\n\n:::\n\n\n## Automation and Custom functions\n-\tCustom functions (very small example, mostly to get used to the syntax)\n\nIt is a frequent issue that when you work with data you would want to perform same several actions with a given dataset. For example, you would want to calculate average for specific set of variables, and then store it in a different dataframe to describe those selected In this case, custom functions are incredibly helpful.\n\nExplore the function below. It stores the averages for `happiness_score`, `life_expectancy` and `freedom` variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_average <- function(dataframe){\n  average_happiness = mean(dataframe$happiness_score)\n  average_expectancy = mean(dataframe$life_expectancy, na.rm = T)\n  average_freedom = mean(dataframe$freedom, na.rm = T)\n  \n  average_output = data.frame(average_happiness, average_expectancy, average_freedom)\n  return(average_output) # This is what the function returns as a result\n}\n```\n:::\n\n\nTry it out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_average(h_report)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  average_happiness average_expectancy average_freedom\n1          5.527572           0.520843       0.6205882\n```\n\n\n:::\n:::\n\n\nNow, we can easily calculate those averages anytime we sample from the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_average(h_report_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  average_happiness average_expectancy average_freedom\n1          5.487167          0.5148118       0.6285374\n```\n\n\n:::\n:::\n\n\n::: callout-note\n## Exercise\n\nCustom functions save a lot of time, especially if you have a non-trivial task. Write a function that would calculate median for `happiness_score`, `life_expectancy` and `freedom` variables in a given dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_median <- function(...){\n  \n  ...\n  \n  median_output = data.frame(...)\n  return(median_output) # This is what the function returns as a result\n}\n```\n:::\n\n\nAnd now try it out!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_median(h_report)\n```\n:::\n\n\nWrite a function that would automate calculation of average happiness in a selected group of countries from regions below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_soviet_countries <- c(\"Kazakhstan\", \"Uzbekistan\", \"Armenia\")\nlatin_america_countries <- c(\"Venezuela\", \"Argentina\", \"Paraguay\")\n```\n:::\n\n\nUse a draft below for your function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naverage_happiness = ...(dataframe, countries_list){\n  \n  dataframe %>% \n    filter(country_name %in% ...) %>%\n    summarize(average = mean(...)) %>%\n    ...()\n  \n}\n\naverage_happiness(h_report, post_soviet_countries)            # Note the difference in (i) countries list\naverage_happiness(h_report_sample, latin_america_countries)   # and (ii) in dataframes we use\n```\n:::\n\n:::\n\n### Apply function\n\nSometimes it is easier to use apply family of functions. For example, we can easily calculate an average using `apply()` function for selected columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  select(happiness_score, log_gdp, social_support) %>%\n  apply(2, mean, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nhappiness_score         log_gdp  social_support \n       5.527572        1.378779        1.134323 \n```\n\n\n:::\n:::\n\n\nMoreover, we can calculate some statistics for observations (i.e., rows), not variables. For example, we can find the minimum value across all variables for a specific country. As you might notice, the second argument in `apply()` function refers to whether we want manipulate rows or columns, 1 or 2 respectively. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  filter(country_name == \"Azerbaijan\") %>%\n  apply(1, min, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"0.1115664\"\n```\n\n\n:::\n:::\n\n\nHowever, most importantly, `apply()` function is compatible with custom functions. For example, you want to standardize the variable so it ranges between 0 to 1. You have the following function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandartize = function(x){\n  (x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T))\n}\n```\n:::\n\n\nNow, you can apply it over a number of variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  select(social_support, freedom) %>%\n  apply(2, standartize) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     social_support   freedom\n[1,]      0.9726830 0.9953240\n[2,]      0.9405118 0.9529831\n[3,]      1.0000000 0.9480820\n[4,]      0.9284272 0.9709709\n[5,]      0.9358299 0.7430091\n[6,]      0.9044380 0.8398647\n```\n\n\n:::\n:::\n\n\n### Map function\n\nIt's quite often the case in programming that the same operation can be done in a multiple ways. Compare the `map()` function from tidyverse with `apply()` function. What is the difference?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  select(happiness_score, log_gdp, social_support) %>% \n  map(mean, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$happiness_score\n[1] 5.527572\n\n$log_gdp\n[1] 1.378779\n\n$social_support\n[1] 1.134323\n```\n\n\n:::\n:::\n\n\nNow, take a look on the following code below. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  select(happiness_score, log_gdp, social_support) %>% \n  map_df(mean, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  happiness_score log_gdp social_support\n            <dbl>   <dbl>          <dbl>\n1            5.53    1.38           1.13\n```\n\n\n:::\n:::\n\nFurthermore, it is compatible with custom functions, too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_report %>%\n  select(happiness_score, log_gdp, social_support) %>% \n  map_df(standartize) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  happiness_score log_gdp social_support\n            <dbl>   <dbl>          <dbl>\n1           1       0.861          0.973\n2           0.974   0.891          0.941\n3           0.964   0.879          1    \n4           0.934   0.877          0.928\n5           0.934   0.842          0.936\n6           0.930   0.888          0.904\n```\n\n\n:::\n:::\n\n\nGenerally, this is an extensive toolkit to learn. But to sum up, `apply()` functions are base to R, and `map()` is an addition from tidyverse. If you are coding in a *tidy* way, then I suggest sticking to the *tidy* functions.\n\nCompare `apply()` and `map()` to `summarize_all()` and `summarize()`.\n\n::: callout-note\n## Exercise\n\nFinish a function that would calculate an average based on two columns, namely `social_support`, `corruption`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naverage_of_two = function(dataset){\n  suppurt_average = mean(dataset$..., na.rm = T)\n  corruptuon_average = mean(..., na.rm = T)\n  \n  average = mean(c(...))\n  return(average)\n}\n\naverage_of_two(h_report)\n```\n:::\n\n\nDo you remember `mtcars` data? Load it\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncars_information <- mtcars\n```\n:::\n\n\nNow, using `apply()` calculate average for each column.\n\n\n::: {.cell}\n\n:::\n\n\nNow, using `map()` calculate median for each column.\n\n\n::: {.cell}\n\n:::\n\n\nUsing the loop draft below to sample 10 cars from cars dataset 15 times. Each time, apply `map_df()` function to calculate the average `mpg` and `wt`, and store the result in the `sample_statistics` dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_statistics = data.frame()\n\nfor(i in 1:...){\n  temporary_sample <- sample_n(cars_information, size = ...)   \n  \n  temporary_sample_average <- temporary_sample %>% \n    select(...) %>% \n    ...(mean)  \n  \n  sample_statistics <- rbind(sample_statistics, ...) \n}\n\nsample_statistics\n```\n:::\n\n\n\n:::\n\n\n## What should you do if you have questions about programming\n\n-   Stack overflow\n\n-   Your peers\n\n-   The team of this math camp\n\n## Check List\n\n<input type=\"checkbox\"/> Standard deviation, PDF and CDF do not scare me\n\n<input type=\"checkbox\"/> I know what sampling is, and on top of this I can use bootstrapping\n\n<input type=\"checkbox\"/> I can understand custom functions and loops\n\n\n## What did we learn over this week in the programming part?\n\n-   What R, RStudio, Quarto and Markdown are\n\n-   Creating and working with objects, vectors and dataframes\n\n-   Logical Operators\n\n-   What Descriptive statistics is\n\n-   Data wrangling, including `mutate()`, `select()`, `filter()`, `summarize()`, `case_when()`, etc.\n\n-   Data Visualization. Now we know what histograms and scatterplots are\n\n-   We can load data in R and work with it!\n\n\n## Sources\n\n-   Georgia State University, Andrew Young School of Policy Studies, Program Evaluation, https://evalsp24.classes.andrewheiss.com/\n\n-   UT Austin, Department of Government, Methods Camp, https://methodscamp.github.io/\n\n-   Harvard University Department of Government, Math Prefresher, https://iqss.github.io/prefresher/",
    "supporting": [
      "04_sampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}